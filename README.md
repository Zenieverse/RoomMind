<div align="center">
<img width="1200" height="475" alt="GHBanner" src="https://github.com/user-attachments/assets/0aa67016-6eaf-458a-adb2-6e31a0763ed6" />
</div>

# Run and deploy your AI Studio app

This contains everything you need to run your app locally.

View your app in AI Studio: https://ai.studio/apps/drive/1C1mOs7q1luYmI-s4sHNYQnlzFWwlDU9v

## Run Locally

**Prerequisites:**  Node.js


1. Install dependencies:
   `npm install`
2. Set the `GEMINI_API_KEY` in [.env.local](.env.local) to your Gemini API key
3. Run the app:
   `npm run dev`

RoomMind â€” Spatial AI for Human-Aware Mixed Reality Spaces
Transform any room into an adaptive, mood-aware, behaviour-responsive environment.

RoomMind XR - a production-ready WebXR spatial intelligence system

Best Implementation of Hand Interactions

MediaPipe Hands tracking with real-time gesture recognition
5 gesture types: Pinch, Point, Open Palm, Fist, None
Gesture-based scene selection and AI triggers
Visual hand skeleton overlay with landmarks
Controller-free natural interactions
ğŸ“· Best Use of Camera Passthrough Access with AI

AI-Powered Passthrough with real-time spatial analysis
Object detection simulation (desk, chair, monitor, etc.)
Context-aware overlays showing detected objects
AI generates environmental insights based on passthrough data
Continuous spatial intelligence updates
ğŸ¥½ Best Immersive Experience Built with Immersive Web SDK

WebXR VR Mode with immersive-vr session support
Desktop XR fallback for non-VR devices
3D spatial visualization with Three.js
Real-time metrics display in VR
Gesture-controlled UI in immersive mode
ğŸ¤– AI Integration Throughout

4 AI-powered features using Poe Embed API:
Real-time insights generation (Claude-Sonnet-4.5)
Optimal scene suggestions (GPT-5.1)
Behavior pattern analysis (Claude-Sonnet-4.5)
Spatial context analysis (Claude-Sonnet-4.5)
ğŸ¯ Complete Feature Set
Multimodal Sensing

âœ… Webcam with pose detection
âœ… Hand tracking (MediaPipe Hands)
âœ… Microphone audio analysis (real-time noise detection)
âœ… Environment simulation (brightness, focus)
Spatial Intelligence

âœ… 6 adaptive scene presets (Focus, Creative, Wellness, Energize, Collaborate, Rest)
âœ… Real-time automations per scene
âœ… Auto-scene suggestions based on state
âœ… Object detection simulation
Data Visualization

âœ… Live Chart.js metrics over time
âœ… 8 real-time sensor metrics
âœ… Historical timeline view
âœ… Data export (JSON)
Immersive Experience

âœ… WebXR VR mode
âœ… Three.js 3D visualization
âœ… Hand gesture controls
âœ… AI passthrough overlays
âœ… Spatial UI in VR
Privacy-First

âœ… All processing on-device
âœ… No cloud uploads
âœ… Local AI inference simulation
âœ… User consent for camera/mic

HOW TO TEST:

Test in Browser - Works immediately in any modern browser
Test on Meta Quest - Open in Quest Browser for VR mode
Enable Camera/Hand Tracking - Grant permissions for full experience
Deploy to GitHub Pages or hosting platform
Submit URL to Meta Quest competition
ğŸ’¡ Key Innovation Points
Multimodal AI Fusion - Combines vision, audio, gesture, and spatial data
Context-Aware Intelligence - Adapts scenes based on behavior patterns
Privacy-Preserving - All inference runs locally
Gesture-Native UI - Designed for controller-free interactions
Production-Ready - Polished UI, error handling, responsive design
The app is fully functional and ready to test! Try enabling the camera, hand tracking, and AI passthrough for the complete experience.

ğŸš€ Overview

RoomMind reimagines spatial intelligence by turning any physical room into a responsive, adaptive environment that understands mood, intention, and interaction patterns. Using computer vision, environmental sensing, and multimodal AI run locally on-device, RoomMind enables real-time insights into how people use the space â€” and augments it with personalized lighting, sound, prompts, and experiences.
Built for mixed reality platforms and next-generation spatial operating systems, RoomMind provides a seamless bridge between digital cognition and physical environments.

âœ¨ Key Features

ğŸ§  Mood & Behaviour Recognition
Real-time emotion/mood inference via multimodal sensing
Activity + posture detection (focus, rest, movement, collaboration)
Adaptive scene transitions based on behavioural signals

ğŸŒˆ Spatial Scene Intelligence
Detects furniture, objects, room type, and spatial layout
Maps movement patterns and micro-interactions
Generates â€œroom statesâ€ that trigger automations

âš¡ Contextual Automations
Lighting, soundscapes, overlays, task prompts
Intelligent reminders tied to user behaviour
Adaptive productivity modes (Deep Focus, Reset, Creative Boost)

ğŸ”’ On-Device Privacy
All inference runs locally on device â€” no cloud storage, no uploads, full user control.

ğŸ—ï¸ How It Works
RoomMind combines:
Vision-based scene understanding (object detection, pose detection, spatial anchors)
Audio and environmental sensing (noise level, brightness, comfort profile)
A multimodal LLM to reason about user behaviour and generate contextual actions
A local rule engine to trigger adaptive scenes
Mixed-reality rendering for visual overlays or environmental enhancements
The system observes â†’ interprets â†’ responds in a continuous loop.

ğŸ› ï¸ Architecture
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚       Sensor Layer        â”‚
 â”‚ Camera â€¢ Mic â€¢ IMU â€¢ Env  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Spatial Understanding    â”‚
 â”‚   (objects, posture, map)  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Multimodal Reasoning     â”‚
 â”‚  (mood â€¢ behaviour â€¢ intent)â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     RoomMind Engine       â”‚
 â”‚  Automations & Scene Logicâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Spatial OS Integration   â”‚
 â”‚ MR Overlays â€¢ Lighting     â”‚
 â”‚ Audio â€¢ App Interactions   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 
ğŸ’¡ Use Cases
Productivity Rooms: Adaptive focus mode with lighting + sound + distraction detection
Wellness Rooms: Mood-aware ambience, stress detection, breathing cues
Collaboration Spaces: Real-time group energy mapping
Home Environments: Personalized â€œenter roomâ€ scenes
XR Workflows: Spatial cognitive assistant that responds to the userâ€™s state

ğŸ§© Tech Stack
Computer Vision: MediaPipe, OpenCV, custom pose + object models
Multimodal AI: On-device LLM
Spatial Graph: Custom mapping layer from camera + sensors
Frontend: MR UI + React Native bridge (or platform-native UI)
Backend: Local inference engine + automation rules

ğŸ“¦ Folder Structure (Recommended)
/roommind
â”œâ”€â”€ /app
â”‚   â”œâ”€â”€ UI
â”‚   â”œâ”€â”€ Scenes
â”‚   â”œâ”€â”€ Overlays
â”œâ”€â”€ /engine
â”‚   â”œâ”€â”€ inference
â”‚   â”œâ”€â”€ rule_engine
â”‚   â”œâ”€â”€ automations
â”œâ”€â”€ /models
â”œâ”€â”€ /scripts
â”œâ”€â”€ /docs
â”‚   â””â”€â”€ README.md

ğŸ”® Future Roadmap
Multi-person group behaviour modeling
Time-series predictive â€œroom forecastingâ€
Richer adaptive scenes (guided focus sessions, wellness rituals)
Marketplace for downloadable RoomMind automation packs
API for third-party mixed reality apps
Cross-device synchronization of room profiles

ğŸ“ License
MIT License â€” free to use, modify, and extend.
